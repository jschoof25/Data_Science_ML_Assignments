---
title: "BIOST 546 - Homework 1"
author: "John Schoof"
date: "1/18/2021"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.height=4)
knitr::opts_knit$set(root.dir = ("C:/Users/jscho/OneDrive - UW/Winter 2021-LAPTOP-7K6NFTGB/BIOST 546/Homework/")) 
getwd()
rm(list=ls())
options(digits = 3) ## Formats output to 3 digits
library(ggplot2)
library(haven)
library(tidyverse)
library(readr)
library(data.table)
library(knitr)
library(glmnet)
library(foreign)
library(sandwich)

```

## Question 1

### Question 1.A. 
The below code reads in the "Medical_Cost" data and checks for any missing data.  There is no missing data.
```{r read data, include=TRUE}
## read in medical cost data
  load("C:/Users/jscho/OneDrive - UW/Winter 2021-LAPTOP-7K6NFTGB/BIOST 546/Homework/Medical_Cost.RData")
  cost <- df
  glimpse(cost)
  which(is.na(cost))
```

### Question 1.B. 
The below scatter plot of charges in dollars on BMI in cm/kg^2.  The blue dots represent smokers while the red dots represent non-smokers.  The plot suggests that there may be effect modification between smoking and BMI.   
```{r scatterplot, include=TRUE}
ggplot(data = cost) + 
  geom_point(aes(x=bmi, y= charges, color = smoker)) + 
  labs(title = "The Relationship Between BMI and Medical Cost Charges",
       x = "BMI (cm/kg^2)",
       y = "Charges ($)")
```
### Question 1.C. Fit the following three models
1. The first model regresses BMI on charges.  
$$
  \text{P}(Charges=Y | BMI=x ) = \beta_0 + \beta_1 * BMI 
$$
  Below is the table of output and scatter plot with the line of best fit.  
```{r question 1c model 1, include = TRUE }
## bmi as only predictor
  mod1c1 <- lm(charges~bmi, data = cost)
  summary(mod1c1)

## 95% CIs with robust standard errors
  coef.1c1 <- summary(mod1c1)$coefficients
  rob.se.1c1 <- sqrt(diag(vcovHC(mod1c1, type = "HC0")))
  conf.int1c1 <- coef.1c1[2,1] + c(0, qnorm(c(0.025, 0.975))) * rob.se.1c1[2]

## Table of results  
  
  #table.mod1c1 <- kable() 
  #"The average change in medical charges that corresponds with a one unit change in BMI."
  
## plot with regrerssion line
  ggplot(data = cost, aes(x=bmi, y= charges)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE) +
    labs(title = "The Relationship Between BMI and Medical Cost Charges",
         x = "BMI",
         y = "Charges")

## training data MSE
  mod1c1.mse <- mean(mod1c1$residuals ^ 2) ## 140,777,900
  
## predict charges for someone with bmi of 32
  pred32.1c1 <- predict(mod1c1, data.frame(bmi = 32)) ## 13,797
  
```
  The Mean Squared Error of the training set is `mod1c1.mse`.  The predicted amount of charges for someone with a BMI of 32 is `pred32.1c1`.

2. The second model regresses medical charges on BMI and smoking status.
$$
  \text{P}(Charges=Y | BMI, Smoker ) = \beta_0 + \beta_1 * BMI + \beta_2 * Smoker
$$
  Below is the table of output and scatter plot with the line of best fit.
```{r question 1c model 2, include = TRUE }
## bmi and smoker as predictors
  mod1c2 <- lm(charges~bmi + smoker, data = cost)
  summary(mod1c2)
  
## Mod 1c2
  coef.1c2 <- summary(mod1c2)$coefficients
  rob.se.1c2 <- sqrt(diag(vcovHC(mod1c2, type = "HC0")))
  conf.int1c2 <-  c((coef.1c2[2,1] + c(0, qnorm(c(0.025, 0.975))) * rob.se.1c2[2]),
                   (coef.1c2[3,1] + c(0, qnorm(c(0.025, 0.975))) * rob.se.1c2[3]))
  #conf.int1c2 <- matrix(conf.int1c2, 
  #                      ncol = 3,
  #                      colnames("Estimate", ))

## Table of results  
  #table.mod1c1 <- kable() 
  #"The average change in medical charges that corresponds with a one unit change in BMI among those with the same smoking status."
  
## Plot with regression lines 
  ggplot(data = cost, aes(x=bmi, y= charges, group = smoker)) + 
    geom_point() + 
    geom_smooth(method = "lm", aes(y = predict(mod1c2, data=cost))) +
    labs(title = "The Relationship Between BMI and Medical Cost Charges",
         x = "BMI",
         y = "Charges")  
  
## training data MSE
  mod1c2.mse <- mean(mod1c2$residuals ^ 2) ## 50,126,126
  
## predict charges for someone with bmi of 32
  pred32.1c2 <- predict(mod1c2, data.frame(bmi = 32, smoker = "yes")) ## 32,551
  
```
  The Mean Squared Error of the training set is `mod1c2.mse`.  The predicted amount of charges for someone with a BMI of 32 is `pred32.1c2`.

3. The third model regresses medical charges on BMI and smoking status and includes an interaction term between BMI and smoking status.
$$
  \text{P}(Charges=Y | BMI, Smoker, BMI*Smoker) = \beta_0 + \beta_1 * BMI + \beta_2 * Smoker + \beta_3 * Smoker * BMI
$$
  Below is the table of output and scatter plot with the line of best fit.
```{r question 1c model 3, include=TRUE}
## include bmi*smoker interaction term
mod1c3 <- lm(charges~bmi*smoker, data = cost)
  summary(mod1c3)
  
## 95 CIs   
  coef.1c3 <- summary(mod1c3)$coefficients
  rob.se.1c3 <- sqrt(diag(vcovHC(mod1c3, type = "HC0")))
  conf.int1c3 <- c((coef.1c3[2,1] + c(0, qnorm(c(0.025, 0.975))) * rob.se.1c3[2]),
                   (coef.1c3[3,1] + c(0, qnorm(c(0.025, 0.975))) * rob.se.1c3[3]),
                   (coef.1c3[4,1] + c(0, qnorm(c(0.025, 0.975))) * rob.se.1c3[4]))

## Table of results
  #table.mod1c1 <- kable() 
  #"The average change in medical charges that corresponds with a one unit change in BMI among those with the same smoking status accounting for interactive effects of BMI and smoking status." 
  
## plot with regrerssion lines
  ggplot(data = cost, aes(x=bmi, y= charges, color = smoker)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE) +
    labs(title = "The Relationship Between BMI and Medical Cost Charges",
         x = "BMI",
         y = "Charges")
  
## training data MSE
  mod1c3.mse <- mean(mod1c3$residuals ^ 2) ##37,841,585
  
## predict charges for someone with bmi of 32
  pred32.1c3 <- predict(mod1c3, data.frame(bmi = 32, smoker = "yes")) ## 33,953 

## by how much would charges change if patient lowered bmi to 28
  pred28.1c3 <- predict(mod1c3, data.frame(bmi = 28, smoker = "yes"))
  pred.diff.1c3 <- pred32.1c3 - pred28.1c3 ## 5,892
  
```
  The Mean Squared Error of the training set is `mod1c2.mse`.  The predicted amount of charges for someone with a BMI of 32 is `pred32.1c2`.  If someone were to lower their BMI from 32 to 28 we would expect to see an average decrease in their medical costs of `pred.diff.1c3`.


### Question 1.D.
The below model regresses medical charges on BMI, smoking status, smoker_bmi30p and the interaction terms of BMI and smoker_bmi30p and smoking status and smoker_bmi30p.
$$
  \text{P}(Charges = Y | BMI, Smoker, SmokerBMI30p, BMI*SmokerBMI30p, Smoker*SmokerBMI30p) = \beta_0 + \beta_1 * BMI + \beta_2 * Smoker + \beta_3 * SmokerBMI30p + \beta_4 * BMI * SmokerBMI30p + \beta_4 * Smoker * SmokerBMI30p
$$
Below is the table of output.
```{r new variable }
## create new var
cost <- cost %>% mutate(smoker_bmi30p = ifelse(smoker == "yes" & bmi > 30, 1, 0))

## regression model
mod1d <- lm(charges ~ bmi + smoker + smoker_bmi30p + bmi*smoker_bmi30p + smoker*smoker_bmi30p, data = cost)
summary(mod1d)

## table of results


```

**Predictor Coefficients**
* **BMI**: Given that the p-value is much less than 0.05, there is strong evidence to reject the null hypothesis that there is no linear association. 
* **Smoker**: Given that the p-value is much less than 0.05, there is strong evidence to reject the null hypothesis that there is no linear association.
* **Smoker_BMI30p**: Given that the p-value is greater than 0.05, evidence to reject the null hypothesis that there is no linear association does not exist.
* **BMI x Smoker_BMI30p**: Given that the p-value is much less than 0.05, there is strong evidence to reject the null hypothesis that there is no linear association.
* **Smoker x Smoker_BMI30p**: Due to multicolieanrity, a coefficient cannot be estimated for this parameter.

**Non-significant Predictor Variables**
* **Smoker_BMI30p**: There is no evidence of a linear association between.  Our sample would not be unusual if the true value of this coefficient was zero. If this variable was removed from the model, the other relationships would likely not change.

## Question 2

### Question 2a - Regression Problems
* **Blood glucose levels as an effect of fat content in diet**: The outcome would be a continuous measure of blood glucose levels and the main predictor of interest would be proportion of calories from fat.  Other predictor variables included in the model would be carbohydrate intake, protein intake, level of physical activity, and common demographic variables.  This data would likely be low-dimensional.
* **Tumor size as an effect of gene presentation**: The outcome would be a continuous measure of tumor size. The predictors would be tens of thousands of genes in the human genome.  This data would be high-dimensional.
* **Health care costs among medicare patients**:  The outcome variable would be a continuous measure of health care cost such as dollars per year.  The data would likely use electronic medical record data and there would be many predictors from many different doctors visit.  This would likely be high dimensional data.

### Question 2b - Classification Problems
* **Diagnosing breast cancer tumor malignancy using medical imaging**: The outcome variable would be a binary measure of tumor malignancy.  The predictors of interest would be each of the pixels in the MRI.  This data would likely be high-dimensional.  
* **Assessing risk factors for myocardial infarction:** 
* 

### Question 2c - Unsupervised Learning
* **Wearable heart monitor data on arrhythmias in the heart to determine most predictive risk factors among younger adults.**:  There is no outcome variables.  All observations are This data would be high-dimensional
* **Neighborhood risk factors **
* **Diagnosing covert stroke with medical imaging.**:  The outcome variable would be binary measure of the presence of a covert stroke or not.  The predictors of interest would be each of the pixels in the CT or MRI as well as risk factors critical for the occurrence of cardiovascular disease and stroke, such as body mass index (BMI), hemoglobin A1c (HbA1c), systolic and diastolic blood pressure, and smoking status. 


## Question 3

### Question 3a

### Question 3b

### Question 3c
A simple linear regression model where the number of parameters equals 1 (p=1) would have a very low variance and a very high level of bias.  The model could be depicted by a straight line.  Therefore, it would have a very low model flexibility and would lie on the far left of the above plot.  A K-nearest neighbor model where the K equals 1 (K=1) would have a very high variance and very low level of bias.  A depiction of the model would be a line that connects all of the points in the data set.  Therefore, it would have a very low model flexibility and would lie on the far left of the above plot.

## Question 4

* If interpretability of the analysis is more important than complexity, one would likely decide to use a linear model instead of a non-parametric model. 
* If the data includes more than four predictor variables, I would always choose a linear model as opposed to a non-parametric model.  This is because with more predictor variables the more flexible, non-parametric model is much more prone to overfitting.  



## Question 5 

### Question 5a
Below is the equation for the fitted model and the table of the regression output.  The interpretation of the intercept is the average medical charges for individuals from the northwest region.  The dummy variable coefficients are interpreted as the difference in average medical charges for individuals from the northeast, southeast, or southwest region and the average medical charges for individuals from the northwest region. 
$$
  \text{P}(Charges=Y | NE01, SE01, SW01) = \beta_0 + \beta_1 * NE01 + \beta_2 * SE01 + \beta_3 * SW01
$$
```{r question 5a }

cost <- cost %>% mutate(northeast01 = ifelse(region == "northeast", 1, 0),
                        southeast01 = ifelse(region == "southeast", 1, 0),
                        southwest01 = ifelse(region == "southwest", 1, 0))
mod5a <- lm(charges~northeast01+southeast01+southwest01, data = cost)
summary(mod5a)
```

### Question 5b
Below is the equation for the fitted model and the table of the regression output.  The intercept has no valid interpretation in this case.  The dummy variable coefficients are interpreted as the difference in average medical charges for individuals from the northeast, southeast, or southwest region and the average medical charges for individuals from the northwest region
```{r question 5b }

cost <- cost %>% mutate(northeast.5 = ifelse(region == "northeast", 0.5, -0.5),
                        southeast.5 = ifelse(region == "southeast", 0.5, -0.5),
                        southwest.5 = ifelse(region == "southwest", 0.5, -0.5))
mod5b <- lm(charges~northeast.5+southeast.5+southwest.5, data = cost)
summary(mod5b)
```













