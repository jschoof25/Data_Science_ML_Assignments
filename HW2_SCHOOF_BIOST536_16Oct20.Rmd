---
title: "BIOST 536 HW2"
author: "John Schoof"
date: "10/13/2020"
output: word_document:
  reference_docx: word_style_reference_01.docx
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
knitr::opts_knit$set(root.dir = ("C:/Users/jscho/OneDrive - UW/FALL 2020-LAPTOP-7K6NFTGB/BIOST 536/HW/")) 

# clean data environment
rm(list=ls())

# load packages
library(haven)
library(tidyverse)
library(sandwich)
library(foreign)
library(survey)
library(stringr)

options(digits = 3) ## Formats output to 3 digits
set.seed(1) # set seed for any randomization that may follow
```

```{r load_data, include=FALSE}
bias <- read.csv("sexbias.csv", header = TRUE)
names(bias) ## Column Names
#bias %>% summarize(n()) ## 4,526 observations 
bias %>% glimpse() ## 4 variables
bias %>% head()
```


```{r clean variables, include=FALSE}
# Make all variable names lowercase
lower_varnames <- function(x) {
    colnames(x) <- tolower(colnames(x))
    x
    }
bias <- lower_varnames(bias)

# Change sex to binary numeric called male 
table(bias$sex, useNA = "ifany")
bias <- bias %>% mutate(male = case_when(sex == "male" ~ 1, 
                                         sex == "female" ~ 0)) 
table(bias$male, useNA = "ifany")

# Change accept to binary numeric
table(bias$accept, useNA = "ifany")
bias <- bias %>% mutate(accept = case_when(accept == "yes" ~ 1, 
                                           accept == "no" ~ 0)) 
table(bias$accept, useNA = "ifany")

# Change major to factor
table(bias$major, useNA = "ifany")
bias <- bias %>% mutate(major = case_when(grepl("A", major) == 1 ~ 1,
                                          grepl("B", major) == 1 ~ 2,
                                          grepl("C", major) == 1 ~ 3,
                                          grepl("D", major) == 1 ~ 4,
                                          grepl("E", major) == 1 ~ 5,
                                          grepl("F", major) == 1 ~ 6))

bias$major <- factor(bias$major,
                        levels = 1:6,
                        labels = c("A", "B", "C", "D", "E", "F"))
table(bias$major, useNA = "ifany")
```

### Question 1
**A major public research university has been accused of discriminating against women in admission to its graduate programs.  A task force randomly selects 6 graduate programs (“majors”) from across the university to investigate the question.  Use the dataset “sexbias” to investigate the following.**

**1a.  For the three variables in the dataset, draw a DAG representing the most appropriate scientific model to approach the question.**

The above DAG represents the scientific model proposed by question 1.  Sex is the exposure variable, which is connected to Accept, the outcome variable.  We are investigating this causal arc.  Major is a third variable which is associated with both Sex and Accept.  

**1b.  Use logistic regression to examine the unadjusted association between sex and acceptance to graduate school.  Summarize the results in language suitable for the task force’s report.**

```{r Question 1b, include=FALSE}
table(bias$accept, useNA = "ifany")
table(bias$male, useNA = "ifany")

mod1b <- glm(accept~male, family = "binomial", data = bias)

summary(mod1b)

exp(coef(mod1b))

# Robust SEs
  coef <- mod1b$coef
  normal_se <- summary(mod1b)$coefficients[, 2]
  rob_se <- sqrt(diag(vcovHC(mod1b, type = "HC0")))
  normal_se
  rob_se

# 95% CI
  conf_int_age <- coef[2] + 
  c(0, qnorm(c(0.025, 0.975))) * rob_se[2]
  conf_int_age
  exp(conf_int_age)

```
$$
  \text{Logit}(Accept = 1 | male ) = \beta_0 + \beta_1 * male
$$
I ran the above unadjusted logistic regression of the log odds of admission to a graduate program based on a sample of  4,526 individuals using robust standard errors.  I estimate that, on average, the odds of a male being accepted are `r exp(coef[2])` (95% CI: `r exp(conf_int_age[2])`, `r exp(conf_int_age[3])`) times greater than the odds of a female being accepted.  This finding is statistically significant at the alpha = 0.05 confidence level.

**1c.  Use logistic regression to examine the association between sex and acceptance to graduate school adjusted for “major”.  Summarize the results in language suitable for the task force’s report.**

```{r Question 1c, include=FALSE}
table(bias$accept, useNA = "ifany")
table(bias$male, useNA = "ifany")
table(bias$major, useNA = "ifany")

mod1c <- glm(accept~male + major, family = "binomial", data = bias)

summary(mod1c)

exp(coef(mod1c))

# Robust SEs
  coef <- mod1c$coef
  normal_se <- summary(mod1c)$coefficients[, 2]
  rob_se <- sqrt(diag(vcovHC(mod1c, type = "HC0")))
  normal_se
  rob_se

# 95% CI
  conf_int_age <- coef[2] + 
  c(0, qnorm(c(0.025, 0.975))) * rob_se[2]
  conf_int_age
  exp(conf_int_age)

```
$$
  \text{Logit}(Accept = 1 | male, major ) = \beta_0 + \beta_1 * male + \beta_2 * major 
$$
I ran the above logistic regression of the log odds of admission to a graduate program based on a sample of  4,526 individuals using robust standard errors.  I estimate that, on average, the odds of a male being accepted are `r exp(coef[2])` **(95% CI:** `r exp(conf_int_age[2])`, `r exp(conf_int_age[3])`) times the odds of a female being accepted for individuals of the same major.  This finding is not statistically significant at the alpha = 0.05 confidence level.

**1d.  Are the results from b and c very different?  Why or why not? (Don’t answer in general terms, answer in terms of this dataset.)**

My findings from 1b and 1c.  The findings from 1b suggest that males are much more likely than females to be accepted to graduate school.  However, after we control for the major in 1c, we find no significant difference in the odds of admission for males and females.  In fact, the findings from 1c suggest that odds of admission for males may actually be lower than that for females when accounting for major.

**1e.  Which analysis best addresses the question of whether the University discriminates against women in graduate school admissions?**

The analysis in 1c, adjusting for major, best addresses this question. It is necessary to adjust for major because males and females apply to different programs in different amounts.  It is possible that overall males appear to be accepted more often than females, but they may just apply to less competitive majors more often.  It is necessary to account for this.

**1f.  Is there any other information you would have liked to have had for this analysis?  E.g. any unmeasured potential confounders?**

Other variables that could be potential confounders might include GRE scores, age, work experience, IQ, earnings in most recent year, whether they have children, home country.

****

### Question 2

**The course CANVAS site has a file of (fictitious) data from a case-control study of lung-cancer examining two exposures, smoking and asbestos.  Fit the saturated logistic regression model discussed in class (“Model A”) to these data.  The model should have a "main effect" for asbestos exposure, and "main effect" for smoking, and an interaction term for asbestos exposure and smoking.  Asbestos is the exposure of interest.**

```{r load data, include=FALSE}
data <- read.csv("asbestos.csv", header = TRUE)
names(data) ## Column Names
data %>% summarize(n()) ## 285 observations 
data %>% glimpse() ## 4 variables
data %>% head()
```
```{r clean data, include=FALSE}
# Make all variable names lowercase
lower_varnames <- function(x) {
    colnames(x) <- tolower(colnames(x))
    x
    }
data <- lower_varnames(data)

# Change smoketo binary numeric called male 
table(data$smoke, useNA = "ifany")
data <- data %>% mutate(smoke = case_when(smoke == "Yes" ~ 1, 
                                          smoke == "No" ~ 0)) 
table(data$smoke, useNA = "ifany")

# Change accept to binary numeric
table(data$asbestos, useNA = "ifany")
data <- data %>% mutate(asbestos = case_when(asbestos == "Yes" ~ 1, 
                                             asbestos == "No" ~ 0)) 
table(data$asbestos, useNA = "ifany")

# Change major to factor
table(data$lungca, useNA = "ifany")
data <- data %>% mutate(lungca = case_when(lungca == "Yes" ~ 1, 
                                           lungca == "No" ~ 0))
table(data$lungca, useNA = "ifany")

```

```{r, question 2 model, include=FALSE}

mod2 <- glm(lungca~asbestos*smoke, family = "binomial", data = data)

summary(mod2)

exp(coef(mod2))

# Robust SEs
  coef <- mod2$coef
  normal_se <- summary(mod2)$coefficients[, 2]
  rob_se <- sqrt(diag(vcovHC(mod2, type = "HC0")))
  normal_se
  rob_se

# 95% CI
  conf_int <- coef[2] + (c(0, qnorm(c(0.025, 0.975))) * rob_se[2])
  conf_int
  exp(conf_int)

# 2c
  exp(coef[2]+coef[4])
  exp(4.09)
  
  library(multcomp)
  lh <- glht(mod2, linfct = t(c(0,1,0,1)))
  confint(lh)
  
```
$$
  \text{Logit}(Lung Cancer = 1 | asbestos, smoke ) = \beta_0 + \beta_1 * asbestos + \beta_2 * smoke + \beta_3 * asbestos * smoke
$$
2a.  For each of the four regression parameters in the model:  what population quantity does the parameter estimate?  If the parameter does not estimate a population quantity, briefly explain why.  

* $\beta_0$ **estimates the log odds of developing lung cancer among individuals who do not smoke and were not exposed to asbestos.**
* $\beta_1$ **estimates the additional log odds of developing lung cancer among individuals who smoke.**
* $\beta_2$ **estimates the additional log odds of developing lung cancer among individuals who were exposed to asbestos.**
* $\beta_3$ **estimates the additional log odds of developing lung cancer among individuals who were exposed to asbestos and smoke.**

2b.  According to the fitted model, what is the OR for asbestos among non-smokers?

$e^{\beta_1} = e^{0.69} = 2.00$

The OR for asbestos among non-smokers is 2.00 (95% CI: 0.68, 6.58).  This means that, among non-smokers, the odds of developing lung cancer for those exposed to asbestos are 2 times greater than those who were unexposed to asbestos on average.  However, this point estimate is not statistically significant at the $\alpha = 0.05$ significance level.

2c.  According to the fitted model, what is the OR for asbestos among smokers?

$e^{\beta_1 + \beta_3} = e^{0.69 + 3.40} = 60$

The OR for asbestos among smokers is 60 (95% CI: 21.4, 168.0).  This means that, among smokers, the odds of developing lung cancer for those exposed to asbestos are 60 times greater than those who were unexposed to asbestos on average.

2d.  Summarize the evidence that smokers and non-smokers have different ORs for asbestos.  Write your answer in a few sentences suitable for a scientific publication.  

I ran the above logistic regression of the log odds of developing lung cancer based on a sample of 285 individuals using robust standard errors.  The large difference between OR estimates for smokers and non-smokers is due to the effect modification of smoking on asbestos.  The interaction term between smoking and asbestos is statistically significant at the $\alpha = 0.05$ significance level.  

2e.  One could instead estimate the OR for asbestos among smokers by fitting a simple logistic regression model using the subset of the data on smokers.  Do this.  Compare your point estimates and confidence intervals here and part c and comment on whether any similarities or differences are to be expected.
```{r, question 2e, include=FALSE}

data.smoke <- data %>% filter(smoke ==1)
data.nosmoke <- data %>% filter(smoke ==0)

mod2e <- glm(lungca~asbestos, family = "binomial", data = data.smoke)

summary(mod2e)

exp(coef(mod2e))

# Robust SEs
  coef <- mod2e$coef
  normal_se <- summary(mod2e)$coefficients[, 2]
  rob_se <- sqrt(diag(vcovHC(mod2e, type = "HC0")))
  normal_se
  rob_se

# 95% CI
  conf_int_age <- coef[2] + 
  c(0, qnorm(c(0.025, 0.975))) * rob_se[2]
  conf_int_age
  exp(conf_int_age)

```

I ran a simple logistic regression model using the subset of the data that smoked.  My analysis found the exact same point estimate and confidence interval for the OR as in part c, 60 (95% CI: 21.4, 168.0).  This is to be expected because this is exactly what happens when we run regression analysis and adjust for a confounding variable.  We use the conditional method and only compare those that have the same value of the confounding variable.


2f.  Use an appropriate logistic regression model to estimate the smoking-adjusted OR for asbestos.  Compare your results with b and c above.
```{r, question 2f, include=FALSE}

mod2f <- glm(lungca~asbestos+smoke, family = "binomial", data = data)

summary(mod2f)

exp(coef(mod2f))

# Robust SEs
  coef <- mod2f$coef
  normal_se <- summary(mod2f)$coefficients[, 2]
  rob_se <- sqrt(diag(vcovHC(mod2f, type = "HC0")))
  normal_se
  rob_se

# 95% CI
  conf_int_age <- coef[2] + 
  c(0, qnorm(c(0.025, 0.975))) * rob_se[2]
  conf_int_age
  exp(conf_int_age)

```
$$
  \text{Logit}(Lung Cancer = 1 | asbestos, smoke ) = \beta_0 + \beta_1 * asbestos + \beta_2 * smoke
$$
I ran the above logistic regression of the log odds of developing lung cancer based on a sample of 285 individuals using robust standard errors.  I estimate that, among those with the same smoking status, the odds of developing lung cancer if one is exposed to asbestos are `r exp(coef[2])` **(95% CI:** `r exp(conf_int_age[2])`, `r exp(conf_int_age[3])`) times higher than those who are not exposed to asbestos.  


2g.  For the model in part f, perform a Wald test and likelihood ratio test of the null hypothesis that the smoking-adjusted odds ratio is 1.
```{r, question 2g, include=FALSE}
data.smoke <- data %>% filter(smoke ==1)
data.nosmoke <- data %>% filter(smoke ==0)
#wald = chi squared test
chisq.test(as.factor(data.smoke$asbestos), as.factor(data.smoke$lungca))
#likelihood ratio test = one sample t test
#with(data, t.test(data.smoke$lungca, data.nosmoke))
```

To test the null hypothesis that smoking-adjusted odds ratio is 1, we perform a chi-squared test of lung cancer on asbestos among non-smokers.  This returns a p-value less than 0.001 which provides evidence against the null hypothesis.  



















