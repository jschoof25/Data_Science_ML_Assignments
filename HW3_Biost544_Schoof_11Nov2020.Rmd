---
title: "HW3 Biost 544"
author: "John Schoof"
date: "11/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.height=4)
knitr::opts_knit$set(root.dir = ("C:/Users/jscho/OneDrive - UW/FALL 2020-LAPTOP-7K6NFTGB/BIOST 544/Lecture Notes")) 
options(digits = 3) ## Formats output to 3 digits
library(ggplot2)
library(dplyr)
library(readr)
library(data.table)
library(knitr)
library(glmnet)

```

```{r readin, warning=FALSE, results='hide', cache.lazy=FALSE, message=FALSE}

NOAH.clinical <- read.csv("../Data/clinical_data.csv", header = TRUE)[,-1]
NOAH.clinical.keep <- NOAH.clinical[,c("centerid", "patid", "necrotic_cells.pct")]

## If you install the data.table package, then the followingg is also fast:
NOAH.expression <- 
  fread("../Data/expression_data_probeID.csv", header = TRUE, sep = ',')[,-1]

## keep the first 100 genes (plus the first two columns, "centerid","patid")
NOAH.exp.keep <- NOAH.expression

if(typeof(NOAH.expression$patid) != typeof(NOAH.clinical$patid)){
  NOAH.exp.keep$centerid <- as.numeric(NOAH.exp.keep$centerid)
  NOAH.exp.keep$patid <- as.numeric(NOAH.exp.keep$patid)
}

NOAH <- inner_join(NOAH.exp.keep, NOAH.clinical.keep, by=c("centerid","patid"))
NOAH.genes <- NOAH %>% select(-c(centerid, patid, necrotic_cells.pct))
```

## Introduction
The purpose of this analysis is investigate the relationship between gene expression, as defined by 54,675 different probe sets, and the percentage of necrotic tissue in a tumor.  The data sets being analyzed includes genetic and clinical data on 152 patients.  

## Methods

#### Association
For each probe, I calculated $R^2$.  I then ranked all of the probes by $R^2$ in descending order and selected the 100 probes with the highest $R^2$.  My original plan was to run permutation tests for each of the 54,675 genes so that I create a null model for $R^2$ for each gene.  I was then going to compare my observed value to that distribution to find the p-value.  I was going to adjust my significance level to 0.0005 to account for multiple testing and only select the genes that the permutation test shows to be significant.  However, the computing time proved to be too long so I set the threshold for the number of genes to include at 100.

```{r part 1, include=TRUE}

## function for correlation
  my.corr.test <- function(probe, outcome){
                    fit.null <- lm(outcome ~ 1)
                    fit.gene <- lm(outcome ~ probe)
                    
                    resids.null <- outcome - predict(fit.null)
                    resids.gene <- outcome - predict(fit.gene)
                    
                    MSE.ratio <- (mean(resids.gene^2))/(mean(resids.null^2))
                    
                    perm.corr <- 1 - MSE.ratio
                    
                    return(as.vector(perm.corr))
                  }

  
## apply my correlation test to all genes
   corr.all.genes <- apply(NOAH.genes, 2, my.corr.test, 
                                           outcome=NOAH$necrotic_cells.pct)
    
    
```


#### Prediction
After identifying the probes that are significantly associated with the outcome, percent of necrotic tissue, I used these probes to build a predictive model using the lasso method.  I fit a lasso model with cross validation.  This means that k-fold cross validation is used to determine which value of lambda minimizes the mean squared error. The model will use penalized estimation to determine which features are most predictive and should be included in the model.

```{r full lasso model}
## restructure the vectors of R squared estimates
## rank them by R squared in descneding order
  r2.genes <- data.frame(as.vector(names(corr.all.genes)),
                        (as.vector(corr.all.genes)))
  colnames(r2.genes) <- c("gene.name", "R2")
  r2.genes <- r2.genes %>% arrange(desc(R2))
  r2.genes <- r2.genes[1:100,]

## create a matrix of genes because that is what glmnet accepts
  test.genes <- as.vector(r2.genes$gene.name)
  test <- NOAH %>% select(necrotic_cells.pct, all_of(test.genes)) %>%
                    as.data.frame()
  gene.mat <- as.matrix(test)

## create a sequence of lambda values for glmnet to test
  nlambda <- 50
  maxlambda <- 3
  my.lambda.seq <- seq(maxlambda, maxlambda*0.01, length.out=nlambda)
 
## fit the lasso model with cross validation to find value
## of lambda that minimizes MSE
  set.seed(2)
  fit.cv <- cv.glmnet(x=gene.mat[,-1], 
                      y=gene.mat[,1], 
                      alpha=1, 
                      lambda=my.lambda.seq)
  plot(fit.cv)
  #coef(fit.cv, s=fit.cv$lambda.min)
  
  table <- as.data.frame(as.matrix(coef(fit.cv, s = fit.cv$lambda.min))) %>% 
                        `colnames<-`("coefficient") %>% 
                        filter(coefficient != 0)

  kable(table, caption = "Table 1", "pipe")
  
## fit the lasso model on the full data set (no cross validation)
  fit.final <- glmnet(x=gene.mat[,-1], 
                      y=gene.mat[,1], 
                      alpha=1, 
                      lambda=fit.cv$lambda.min)
  
   
```

## Results
As seen in the plot of the Mean-Squared Error on Log($\lambda$) my lasso model included 52 genes in the model that best predicts the percent of tumor that is made up of necrotic tissue.  Table 1 shows the coefficients of these 52 genes in the model that best predicts.  The value of lambda at which the MSE is minimized is 0.151.
After fitting the lasso model using cross validation to identify which of the 100 most associated genes are most predictive, the final step was to fit the model on my full dataset using the value of lambda that minimizes the mean squared error.  We see that these gene-expression values are associated with percentage of necrotic tissue in tumors.  

